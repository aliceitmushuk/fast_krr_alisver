{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeops.torch import LazyTensor\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f30c00a1490>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_nys_appx(K, n, r, device):\n",
    "    # Calculate sketch\n",
    "    Phi = torch.randn((n, r), device=device) / (n ** 0.5)\n",
    "    Phi = torch.linalg.qr(Phi, mode='reduced')[0]\n",
    "\n",
    "    Y = K @ Phi\n",
    "\n",
    "    # Calculate shift\n",
    "    shift = torch.finfo(Y.dtype).eps\n",
    "    Y_shifted = Y + shift * Phi\n",
    "\n",
    "    # Calculate Phi^T * K * Phi (w/ shift) for Cholesky\n",
    "    choleskytarget = torch.mm(Phi.t(), Y_shifted)\n",
    "\n",
    "    # Perform Cholesky decomposition\n",
    "    C = torch.linalg.cholesky(choleskytarget)\n",
    "\n",
    "    B = torch.linalg.solve_triangular(C, Y_shifted, upper=False, left=False)\n",
    "    U, S, _ = torch.linalg.svd(B, full_matrices=False)\n",
    "\n",
    "    return U, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(K, b, lambd, a):\n",
    "    return 1/2 * torch.dot(a, K @ a) + lambd / 2 * torch.dot(a, a) \\\n",
    "            - torch.dot(b, a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(K, b, lambd, a0, rho, eta, r, max_iter, device):\n",
    "    n = K.shape[0]\n",
    "    U, S = rand_nys_appx(K, n, r, device)\n",
    "\n",
    "    print(S)\n",
    "\n",
    "    plt.semilogy(S.cpu().numpy())\n",
    "    plt.show()\n",
    "\n",
    "    a = a0.clone()\n",
    "    for i in range(max_iter):\n",
    "        g = K @ a + lambd * a - b\n",
    "        UTg = U.t() @ g\n",
    "        dir = U @ (UTg / (S + rho)) + 1/rho * (g - U @ UTg)\n",
    "        a -= eta * dir\n",
    "\n",
    "        print(f\"iter {i}, loss {loss(K, b, lambd, a)}\")\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocks(n, B):\n",
    "    # Permute the indices\n",
    "    idx = torch.randperm(n)\n",
    "\n",
    "    # Partition the indices into B blocks of roughly equal size\n",
    "    # Do this by first computing the block size then making a list of block sizes\n",
    "    block_size = n // B\n",
    "    remainder = n % B\n",
    "    sizes = [block_size] * B\n",
    "\n",
    "    for i in range(remainder):\n",
    "        sizes[i] += 1\n",
    "\n",
    "    blocks = torch.split(idx, sizes)\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RBF kernel only -- need to generalize\n",
    "def bcd(x, sigma, b, lambd, a0, rho, eta, B, r, max_iter, device):\n",
    "    x_i = LazyTensor(x[:, None, :])\n",
    "    x_j = LazyTensor(x[None, :, :])\n",
    "    D = ((x_i - x_j) ** 2).sum(dim=2)\n",
    "    K = (-D / (2 * sigma ** 2)).exp()\n",
    "\n",
    "    n = K.shape[0]\n",
    "    blocks = get_blocks(n, B)\n",
    "    block_preconds = []\n",
    "\n",
    "    # Compute randomized Nystrom approximation corresponding to each block\n",
    "    for block in blocks:\n",
    "        xb_i = LazyTensor(x[block][:, None, :])\n",
    "        xb_j = LazyTensor(x[block][None, :, :])\n",
    "        Db = ((xb_i - xb_j) ** 2).sum(dim=2)\n",
    "        Kb = (-Db / (2 * sigma ** 2)).exp()\n",
    "\n",
    "        U, S = rand_nys_appx(Kb, block.shape[0], r, device)\n",
    "        block_preconds.append((U, S))\n",
    "\n",
    "    a = a0.clone()\n",
    "    for i in range(max_iter):\n",
    "        # Randomly select a block\n",
    "        block_idx = torch.randint(B, (1,))\n",
    "\n",
    "        # Get the block and its corresponding preconditioner\n",
    "        block = blocks[block_idx]\n",
    "        U, S = block_preconds[block_idx]\n",
    "\n",
    "        # Compute block gradient\n",
    "        # xb_j = LazyTensor(x[block][None, :, :])\n",
    "        # DbnT = ((x_i - xb_j) ** 2).sum(dim=2)\n",
    "        # KbnT = (-DbnT / (2 * sigma ** 2)).exp()\n",
    "\n",
    "        # gb = n/block.shape[0] * (KbnT @ a[block] + lambd * a[block] - b[block])\n",
    "\n",
    "        xb_i = LazyTensor(x[block][:, None, :])\n",
    "        Dbn = ((xb_i - x_j) ** 2).sum(dim=2)\n",
    "        Kbn = (-Dbn / (2 * sigma ** 2)).exp()\n",
    "\n",
    "        gb = n/block.shape[0] * (Kbn @ a + lambd * a[block] - b[block])\n",
    "\n",
    "        # Apply preconditioner\n",
    "        UTgb = U.t() @ gb\n",
    "        dir = U @ (UTgb / (S + rho)) + 1/rho * (gb - U @ UTgb)\n",
    "\n",
    "        # Update block\n",
    "        a[block] -= eta * dir\n",
    "\n",
    "        print(f\"iter {i}, loss {loss(K, b, lambd, a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sizes\n",
    "N_x = 1000000  # Number of rows in x\n",
    "\n",
    "# Generate random data\n",
    "x = torch.randn(N_x, 3, dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "# Define your LazyTensors\n",
    "x_i = LazyTensor(x[:, None, :])  # Shape (N_x, 1, 3)\n",
    "x_j = LazyTensor(x[None, :, :])  # Shape (1, N_y, 3)\n",
    "\n",
    "# Compute the Gaussian kernel matrix\n",
    "sigma = 1.0  # Kernel width\n",
    "D_ij = ((x_i - x_j) ** 2).sum(dim=2)  # Squared Euclidean distances\n",
    "K_ij = (- D_ij / (2 * sigma ** 2)).exp()  # Gaussian kernel matrix, still lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.randn(N_x, dtype=torch.float32).to(device)\n",
    "lambd = 0.1 #(10 ** -2) / N_x\n",
    "a0 = torch.zeros(N_x, dtype=torch.float32).to(device)\n",
    "rho = 1\n",
    "eta = 0.01\n",
    "r = 300\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4991643.5000, device='cuda:1', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Ground truth\n",
    "a_star = K_ij.solve(torch.unsqueeze(b, 1), alpha=lambd)\n",
    "print(loss(K_ij, b, lambd, torch.squeeze(a_star)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     a = gd(K_ij, b, lambd, a0, rho, eta, r, max_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n",
      "iter 0, loss -9435.8017578125\n",
      "iter 1, loss -18779.904296875\n",
      "iter 2, loss -28177.6796875\n",
      "iter 3, loss -37644.4765625\n",
      "iter 4, loss -46974.96875\n",
      "iter 5, loss -56123.78125\n",
      "iter 6, loss -65160.3671875\n",
      "iter 7, loss -74543.8359375\n",
      "iter 8, loss -83681.4921875\n",
      "iter 9, loss -92923.28125\n",
      "iter 10, loss -102416.34375\n",
      "iter 11, loss -111765.9453125\n",
      "iter 12, loss -119321.8125\n",
      "iter 13, loss -128679.390625\n",
      "iter 14, loss -138158.234375\n",
      "iter 15, loss -147446.40625\n",
      "iter 16, loss -156742.265625\n",
      "iter 17, loss -166303.53125\n",
      "iter 18, loss -175830.234375\n",
      "iter 19, loss -185124.921875\n",
      "iter 20, loss -194801.046875\n",
      "iter 21, loss -204175.421875\n",
      "iter 22, loss -213548.078125\n",
      "iter 23, loss -223000.578125\n",
      "iter 24, loss -230531.328125\n",
      "iter 25, loss -239880.59375\n",
      "iter 26, loss -249301.125\n",
      "iter 27, loss -256869.453125\n",
      "iter 28, loss -266231.46875\n",
      "iter 29, loss -275550.71875\n",
      "iter 30, loss -283222.0\n",
      "iter 31, loss -292715.9375\n",
      "iter 32, loss -298827.53125\n",
      "iter 33, loss -306238.46875\n",
      "iter 34, loss -315866.78125\n",
      "iter 35, loss -323449.03125\n",
      "iter 36, loss -331123.375\n",
      "iter 37, loss -337257.0625\n",
      "iter 38, loss -342194.1875\n",
      "iter 39, loss -351487.3125\n",
      "iter 40, loss -360998.4375\n",
      "iter 41, loss -370355.0\n",
      "iter 42, loss -376488.125\n",
      "iter 43, loss -385947.375\n",
      "iter 44, loss -395387.75\n",
      "iter 45, loss -403136.875\n",
      "iter 46, loss -412594.0625\n",
      "iter 47, loss -420129.8125\n",
      "iter 48, loss -429182.71875\n",
      "iter 49, loss -438638.3125\n",
      "iter 50, loss -447981.09375\n",
      "iter 51, loss -457352.90625\n",
      "iter 52, loss -464932.03125\n",
      "iter 53, loss -474249.34375\n",
      "iter 54, loss -480477.15625\n",
      "iter 55, loss -489691.46875\n",
      "iter 56, loss -499105.40625\n",
      "iter 57, loss -508456.78125\n",
      "iter 58, loss -514728.28125\n",
      "iter 59, loss -522304.9375\n",
      "iter 60, loss -531701.6875\n",
      "iter 61, loss -541160.375\n",
      "iter 62, loss -546202.125\n",
      "iter 63, loss -555737.0\n",
      "iter 64, loss -564985.875\n",
      "iter 65, loss -570999.1875\n",
      "iter 66, loss -580328.4375\n",
      "iter 67, loss -589723.375\n",
      "iter 68, loss -599363.1875\n",
      "iter 69, loss -606966.5625\n",
      "iter 70, loss -616432.0\n",
      "iter 71, loss -625661.125\n",
      "iter 72, loss -633367.125\n",
      "iter 73, loss -641051.625\n",
      "iter 74, loss -650523.25\n",
      "iter 75, loss -658112.25\n",
      "iter 76, loss -667849.0625\n",
      "iter 77, loss -675682.0\n",
      "iter 78, loss -685276.8125\n",
      "iter 79, loss -694523.25\n",
      "iter 80, loss -704005.8125\n",
      "iter 81, loss -713262.875\n",
      "iter 82, loss -722823.0\n",
      "iter 83, loss -729064.6875\n",
      "iter 84, loss -738378.0625\n",
      "iter 85, loss -747712.4375\n",
      "iter 86, loss -755428.3125\n",
      "iter 87, loss -762842.0\n",
      "iter 88, loss -772291.0\n",
      "iter 89, loss -779869.3125\n",
      "iter 90, loss -786012.3125\n",
      "iter 91, loss -793697.0625\n",
      "iter 92, loss -801038.375\n",
      "iter 93, loss -808592.6875\n",
      "iter 94, loss -817805.5\n",
      "iter 95, loss -827111.5625\n",
      "iter 96, loss -832090.375\n",
      "iter 97, loss -839736.1875\n",
      "iter 98, loss -847370.625\n",
      "iter 99, loss -852361.875\n",
      "iter 100, loss -860002.25\n",
      "iter 101, loss -867715.625\n",
      "iter 102, loss -877257.6875\n",
      "iter 103, loss -884920.4375\n",
      "iter 104, loss -894284.5625\n",
      "iter 105, loss -903676.375\n",
      "iter 106, loss -907774.9375\n",
      "iter 107, loss -915361.8125\n",
      "iter 108, loss -922852.3125\n",
      "iter 109, loss -928961.0\n",
      "iter 110, loss -936493.0\n",
      "iter 111, loss -944151.9375\n",
      "iter 112, loss -949210.3125\n",
      "iter 113, loss -958458.5\n",
      "iter 114, loss -963402.0\n",
      "iter 115, loss -971061.375\n",
      "iter 116, loss -975100.6875\n",
      "iter 117, loss -982585.3125\n",
      "iter 118, loss -990082.125\n",
      "iter 119, loss -997898.25\n",
      "iter 120, loss -1005462.5625\n",
      "iter 121, loss -1014668.75\n",
      "iter 122, loss -1023889.8125\n",
      "iter 123, loss -1033218.75\n",
      "iter 124, loss -1040888.25\n",
      "iter 125, loss -1050327.0\n",
      "iter 126, loss -1057906.125\n",
      "iter 127, loss -1065466.875\n",
      "iter 128, loss -1074838.75\n",
      "iter 129, loss -1080833.0\n",
      "iter 130, loss -1087096.875\n",
      "iter 131, loss -1096358.875\n",
      "iter 132, loss -1104028.125\n",
      "iter 133, loss -1110171.25\n",
      "iter 134, loss -1119585.625\n",
      "iter 135, loss -1127148.375\n",
      "iter 136, loss -1133318.125\n",
      "iter 137, loss -1139574.375\n",
      "iter 138, loss -1149012.375\n",
      "iter 139, loss -1155123.875\n",
      "iter 140, loss -1164680.375\n",
      "iter 141, loss -1170897.875\n",
      "iter 142, loss -1177104.0\n",
      "iter 143, loss -1182087.5\n",
      "iter 144, loss -1186102.625\n",
      "iter 145, loss -1192311.25\n",
      "iter 146, loss -1200110.0\n",
      "iter 147, loss -1206506.75\n",
      "iter 148, loss -1209787.5\n",
      "iter 149, loss -1215912.25\n",
      "iter 150, loss -1223593.625\n",
      "iter 151, loss -1231070.75\n",
      "iter 152, loss -1238671.75\n",
      "iter 153, loss -1246305.375\n",
      "iter 154, loss -1252481.375\n",
      "iter 155, loss -1257508.125\n",
      "iter 156, loss -1266955.875\n",
      "iter 157, loss -1274456.125\n",
      "iter 158, loss -1280616.75\n",
      "iter 159, loss -1285560.75\n",
      "iter 160, loss -1290593.5\n",
      "iter 161, loss -1299940.375\n",
      "iter 162, loss -1309479.0\n",
      "iter 163, loss -1318869.0\n",
      "iter 164, loss -1325028.75\n",
      "iter 165, loss -1329901.5\n",
      "iter 166, loss -1333976.625\n",
      "iter 167, loss -1338955.875\n",
      "iter 168, loss -1344896.625\n",
      "iter 169, loss -1352671.875\n",
      "iter 170, loss -1362166.0\n",
      "iter 171, loss -1366119.25\n",
      "iter 172, loss -1370151.375\n",
      "iter 173, loss -1379335.5\n",
      "iter 174, loss -1386897.5\n",
      "iter 175, loss -1394395.25\n",
      "iter 176, loss -1402015.125\n",
      "iter 177, loss -1411433.25\n",
      "iter 178, loss -1419098.125\n",
      "iter 179, loss -1428706.5\n",
      "iter 180, loss -1434810.875\n",
      "iter 181, loss -1440016.75\n",
      "iter 182, loss -1446085.0\n",
      "iter 183, loss -1451020.25\n",
      "iter 184, loss -1457148.875\n",
      "iter 185, loss -1464843.0\n",
      "iter 186, loss -1470919.75\n",
      "iter 187, loss -1474247.625\n",
      "iter 188, loss -1481868.625\n",
      "iter 189, loss -1489480.25\n",
      "iter 190, loss -1498739.875\n",
      "iter 191, loss -1504960.125\n",
      "iter 192, loss -1507618.25\n",
      "iter 193, loss -1513732.25\n",
      "iter 194, loss -1521251.625\n",
      "iter 195, loss -1530720.0\n",
      "iter 196, loss -1538335.5\n",
      "iter 197, loss -1544559.25\n",
      "iter 198, loss -1549653.625\n",
      "iter 199, loss -1555851.875\n",
      "iter 200, loss -1562087.75\n",
      "iter 201, loss -1564249.125\n",
      "iter 202, loss -1571862.625\n",
      "iter 203, loss -1576836.5\n",
      "iter 204, loss -1584572.75\n",
      "iter 205, loss -1587785.75\n",
      "iter 206, loss -1595379.0\n",
      "iter 207, loss -1601608.5\n",
      "iter 208, loss -1609314.875\n",
      "iter 209, loss -1618806.0\n",
      "iter 210, loss -1622837.625\n",
      "iter 211, loss -1630534.875\n",
      "iter 212, loss -1635507.0\n",
      "iter 213, loss -1641652.125\n",
      "iter 214, loss -1647724.5\n",
      "iter 215, loss -1652799.25\n",
      "iter 216, loss -1658883.75\n",
      "iter 217, loss -1663874.625\n",
      "iter 218, loss -1667907.0\n",
      "iter 219, loss -1672024.0\n",
      "iter 220, loss -1677049.0\n",
      "iter 221, loss -1684382.875\n",
      "iter 222, loss -1693503.5\n",
      "iter 223, loss -1699777.375\n",
      "iter 224, loss -1704818.25\n",
      "iter 225, loss -1711003.875\n",
      "iter 226, loss -1717063.0\n",
      "iter 227, loss -1724849.375\n",
      "iter 228, loss -1729864.0\n",
      "iter 229, loss -1737484.125\n",
      "iter 230, loss -1741530.375\n",
      "iter 231, loss -1745616.0\n",
      "iter 232, loss -1753002.625\n",
      "iter 233, loss -1759236.125\n",
      "iter 234, loss -1767076.375\n",
      "iter 235, loss -1774522.75\n",
      "iter 236, loss -1782074.0\n",
      "iter 237, loss -1785427.0\n",
      "iter 238, loss -1794610.25\n",
      "iter 239, loss -1800757.0\n",
      "iter 240, loss -1806980.0\n",
      "iter 241, loss -1814384.875\n",
      "iter 242, loss -1820596.875\n",
      "iter 243, loss -1824732.5\n",
      "iter 244, loss -1832195.125\n",
      "iter 245, loss -1835455.5\n",
      "iter 246, loss -1841442.5\n",
      "iter 247, loss -1846420.25\n",
      "iter 248, loss -1851385.0\n",
      "iter 249, loss -1860688.5\n",
      "iter 250, loss -1866879.0\n",
      "iter 251, loss -1873039.0\n",
      "iter 252, loss -1879261.75\n",
      "iter 253, loss -1887006.75\n",
      "iter 254, loss -1893251.0\n",
      "iter 255, loss -1896568.25\n",
      "iter 256, loss -1898316.75\n",
      "iter 257, loss -1904491.125\n",
      "iter 258, loss -1907761.375\n",
      "iter 259, loss -1916981.25\n",
      "iter 260, loss -1924471.0\n",
      "iter 261, loss -1929412.25\n",
      "iter 262, loss -1937159.625\n",
      "iter 263, loss -1941285.75\n",
      "iter 264, loss -1946297.875\n",
      "iter 265, loss -1950335.875\n",
      "iter 266, loss -1956597.375\n",
      "iter 267, loss -1962813.25\n",
      "iter 268, loss -1966855.5\n",
      "iter 269, loss -1972899.625\n",
      "iter 270, loss -1977893.5\n",
      "iter 271, loss -1981158.5\n",
      "iter 272, loss -1987238.375\n",
      "iter 273, loss -1994819.375\n",
      "iter 274, loss -2002353.75\n",
      "iter 275, loss -2008391.125\n",
      "iter 276, loss -2014715.625\n",
      "iter 277, loss -2019585.0\n",
      "iter 278, loss -2022300.625\n",
      "iter 279, loss -2027227.25\n",
      "iter 280, loss -2034713.0\n",
      "iter 281, loss -2038774.5\n",
      "iter 282, loss -2046516.625\n",
      "iter 283, loss -2052792.125\n",
      "iter 284, loss -2057819.875\n",
      "iter 285, loss -2067180.75\n",
      "iter 286, loss -2071206.25\n",
      "iter 287, loss -2076266.75\n",
      "iter 288, loss -2078870.625\n",
      "iter 289, loss -2088149.375\n",
      "iter 290, loss -2091492.0\n",
      "iter 291, loss -2094188.125\n",
      "iter 292, loss -2100339.5\n",
      "iter 293, loss -2107816.25\n",
      "iter 294, loss -2113980.75\n",
      "iter 295, loss -2118028.0\n",
      "iter 296, loss -2123119.5\n",
      "iter 297, loss -2129457.0\n",
      "iter 298, loss -2132720.5\n",
      "iter 299, loss -2142065.5\n",
      "iter 300, loss -2144711.0\n",
      "iter 301, loss -2149700.75\n",
      "iter 302, loss -2154697.5\n",
      "iter 303, loss -2157974.0\n",
      "iter 304, loss -2162017.0\n",
      "iter 305, loss -2165289.0\n",
      "iter 306, loss -2167944.0\n",
      "iter 307, loss -2172017.5\n",
      "iter 308, loss -2178325.25\n",
      "iter 309, loss -2182405.5\n",
      "iter 310, loss -2185681.25\n",
      "iter 311, loss -2190684.5\n",
      "iter 312, loss -2196797.25\n",
      "iter 313, loss -2202930.75\n",
      "iter 314, loss -2205595.0\n",
      "iter 315, loss -2210628.25\n",
      "iter 316, loss -2213930.0\n",
      "iter 317, loss -2223211.75\n",
      "iter 318, loss -2229484.5\n",
      "iter 319, loss -2232760.25\n",
      "iter 320, loss -2242144.5\n",
      "iter 321, loss -2247195.5\n",
      "iter 322, loss -2250464.0\n",
      "iter 323, loss -2255366.75\n",
      "iter 324, loss -2262852.5\n",
      "iter 325, loss -2267832.75\n",
      "iter 326, loss -2271856.75\n",
      "iter 327, loss -2275815.75\n",
      "iter 328, loss -2279095.0\n",
      "iter 329, loss -2285128.5\n",
      "iter 330, loss -2289953.75\n",
      "iter 331, loss -2297524.25\n",
      "iter 332, loss -2303705.25\n",
      "iter 333, loss -2306361.25\n",
      "iter 334, loss -2314028.75\n",
      "iter 335, loss -2316714.25\n",
      "iter 336, loss -2321712.25\n",
      "iter 337, loss -2327770.25\n",
      "iter 338, loss -2332708.0\n",
      "iter 339, loss -2336771.75\n",
      "iter 340, loss -2340121.5\n",
      "iter 341, loss -2345173.25\n",
      "iter 342, loss -2352726.5\n",
      "iter 343, loss -2357623.75\n",
      "iter 344, loss -2361761.25\n",
      "iter 345, loss -2365059.25\n",
      "iter 346, loss -2372634.0\n",
      "iter 347, loss -2374791.0\n",
      "iter 348, loss -2379827.5\n",
      "iter 349, loss -2384911.75\n",
      "iter 350, loss -2388876.5\n",
      "iter 351, loss -2395098.5\n",
      "iter 352, loss -2400110.25\n",
      "iter 353, loss -2402770.5\n",
      "iter 354, loss -2405436.0\n",
      "iter 355, loss -2408118.0\n",
      "iter 356, loss -2412146.5\n",
      "iter 357, loss -2416141.75\n",
      "iter 358, loss -2422221.0\n",
      "iter 359, loss -2425479.5\n",
      "iter 360, loss -2433051.75\n",
      "iter 361, loss -2438088.0\n",
      "iter 362, loss -2440264.5\n",
      "iter 363, loss -2442423.75\n",
      "iter 364, loss -2445714.5\n",
      "iter 365, loss -2453180.25\n",
      "iter 366, loss -2459243.75\n",
      "iter 367, loss -2461908.0\n",
      "iter 368, loss -2466907.5\n",
      "iter 369, loss -2474432.25\n",
      "iter 370, loss -2477095.0\n",
      "iter 371, loss -2479747.5\n",
      "iter 372, loss -2484639.0\n",
      "iter 373, loss -2488694.0\n",
      "iter 374, loss -2492774.5\n",
      "iter 375, loss -2496035.75\n",
      "iter 376, loss -2502260.5\n",
      "iter 377, loss -2508309.75\n",
      "iter 378, loss -2514624.25\n",
      "iter 379, loss -2518737.0\n",
      "iter 380, loss -2520929.75\n",
      "iter 381, loss -2525016.0\n",
      "iter 382, loss -2527671.75\n",
      "iter 383, loss -2529422.25\n",
      "iter 384, loss -2532142.25\n",
      "iter 385, loss -2538270.75\n",
      "iter 386, loss -2544397.5\n",
      "iter 387, loss -2549307.0\n",
      "iter 388, loss -2556832.25\n",
      "iter 389, loss -2560101.5\n",
      "iter 390, loss -2566267.25\n",
      "iter 391, loss -2571300.75\n",
      "iter 392, loss -2574607.0\n",
      "iter 393, loss -2576755.0\n",
      "iter 394, loss -2580066.0\n",
      "iter 395, loss -2582224.5\n",
      "iter 396, loss -2587143.5\n",
      "iter 397, loss -2590422.5\n",
      "iter 398, loss -2594502.5\n",
      "iter 399, loss -2602055.75\n",
      "iter 400, loss -2608147.25\n",
      "iter 401, loss -2610327.5\n",
      "iter 402, loss -2613023.0\n",
      "iter 403, loss -2616374.5\n",
      "iter 404, loss -2621349.75\n",
      "iter 405, loss -2625419.5\n",
      "iter 406, loss -2629523.75\n",
      "iter 407, loss -2632729.25\n",
      "iter 408, loss -2634476.0\n",
      "iter 409, loss -2639432.0\n",
      "iter 410, loss -2644529.75\n",
      "iter 411, loss -2649392.75\n",
      "iter 412, loss -2657043.0\n",
      "iter 413, loss -2659250.0\n",
      "iter 414, loss -2661002.0\n",
      "iter 415, loss -2665045.0\n",
      "iter 416, loss -2670172.0\n",
      "iter 417, loss -2675146.5\n",
      "iter 418, loss -2682619.25\n",
      "iter 419, loss -2686723.5\n",
      "iter 420, loss -2691812.5\n",
      "iter 421, loss -2693233.0\n",
      "iter 422, loss -2699361.0\n",
      "iter 423, loss -2700789.5\n",
      "iter 424, loss -2710223.75\n",
      "iter 425, loss -2715143.75\n",
      "iter 426, loss -2716932.0\n",
      "iter 427, loss -2721840.0\n",
      "iter 428, loss -2723951.0\n",
      "iter 429, loss -2726622.5\n",
      "iter 430, loss -2734191.5\n",
      "iter 431, loss -2740315.75\n",
      "iter 432, loss -2742468.5\n",
      "iter 433, loss -2746462.0\n",
      "iter 434, loss -2750598.25\n",
      "iter 435, loss -2754590.25\n",
      "iter 436, loss -2759703.0\n",
      "iter 437, loss -2764605.25\n",
      "iter 438, loss -2769534.0\n",
      "iter 439, loss -2772239.5\n",
      "iter 440, loss -2778369.75\n",
      "iter 441, loss -2784518.5\n",
      "iter 442, loss -2790878.5\n",
      "iter 443, loss -2794914.25\n",
      "iter 444, loss -2798988.75\n",
      "iter 445, loss -2806571.0\n",
      "iter 446, loss -2811704.0\n",
      "iter 447, loss -2813876.5\n",
      "iter 448, loss -2817814.0\n",
      "iter 449, loss -2821113.5\n",
      "iter 450, loss -2822854.0\n",
      "iter 451, loss -2828985.5\n",
      "iter 452, loss -2832170.75\n",
      "iter 453, loss -2835527.5\n",
      "iter 454, loss -2836949.0\n",
      "iter 455, loss -2844601.25\n",
      "iter 456, loss -2846761.5\n",
      "iter 457, loss -2856072.25\n",
      "iter 458, loss -2858251.0\n",
      "iter 459, loss -2859410.25\n",
      "iter 460, loss -2864450.75\n",
      "iter 461, loss -2865378.25\n",
      "iter 462, loss -2867588.5\n",
      "iter 463, loss -2870829.5\n",
      "iter 464, loss -2874880.5\n",
      "iter 465, loss -2878877.5\n",
      "iter 466, loss -2882213.5\n",
      "iter 467, loss -2887340.5\n",
      "iter 468, loss -2890671.0\n",
      "iter 469, loss -2893368.5\n",
      "iter 470, loss -2897383.75\n",
      "iter 471, loss -2901363.0\n",
      "iter 472, loss -2906407.0\n",
      "iter 473, loss -2908152.75\n",
      "iter 474, loss -2910872.5\n",
      "iter 475, loss -2914110.5\n",
      "iter 476, loss -2915889.75\n",
      "iter 477, loss -2919104.5\n",
      "iter 478, loss -2922342.0\n",
      "iter 479, loss -2924104.5\n",
      "iter 480, loss -2930198.75\n",
      "iter 481, loss -2934295.5\n",
      "iter 482, loss -2936981.0\n",
      "iter 483, loss -2939627.25\n",
      "iter 484, loss -2941377.75\n",
      "iter 485, loss -2945354.0\n",
      "iter 486, loss -2949325.25\n",
      "iter 487, loss -2952572.25\n",
      "iter 488, loss -2955799.75\n",
      "iter 489, loss -2958505.5\n",
      "iter 490, loss -2961126.5\n",
      "iter 491, loss -2965244.25\n",
      "iter 492, loss -2968546.5\n",
      "iter 493, loss -2971780.0\n",
      "iter 494, loss -2975080.0\n",
      "iter 495, loss -2979295.5\n",
      "iter 496, loss -2981897.5\n",
      "iter 497, loss -2984582.0\n",
      "iter 498, loss -2986778.5\n",
      "iter 499, loss -2990745.0\n",
      "iter 500, loss -2993420.5\n",
      "iter 501, loss -3001056.75\n",
      "iter 502, loss -3002833.0\n",
      "iter 503, loss -3006817.5\n",
      "iter 504, loss -3013047.25\n",
      "iter 505, loss -3015716.75\n",
      "iter 506, loss -3019754.75\n",
      "iter 507, loss -3021931.25\n",
      "iter 508, loss -3025910.5\n",
      "iter 509, loss -3030854.25\n",
      "iter 510, loss -3038397.5\n",
      "iter 511, loss -3039824.0\n",
      "iter 512, loss -3043038.75\n",
      "iter 513, loss -3046329.5\n",
      "iter 514, loss -3047767.75\n",
      "iter 515, loss -3052742.5\n",
      "iter 516, loss -3055986.5\n",
      "iter 517, loss -3058593.75\n",
      "iter 518, loss -3062622.0\n",
      "iter 519, loss -3066637.75\n",
      "iter 520, loss -3069299.25\n",
      "iter 521, loss -3071445.75\n",
      "iter 522, loss -3072896.75\n",
      "iter 523, loss -3077980.0\n",
      "iter 524, loss -3079761.25\n",
      "iter 525, loss -3084739.0\n",
      "iter 526, loss -3090785.5\n",
      "iter 527, loss -3095752.75\n",
      "iter 528, loss -3096918.5\n",
      "iter 529, loss -3101057.5\n",
      "iter 530, loss -3102482.5\n",
      "iter 531, loss -3105762.0\n",
      "iter 532, loss -3107891.0\n",
      "iter 533, loss -3110484.0\n",
      "iter 534, loss -3112659.25\n",
      "iter 535, loss -3114832.25\n",
      "iter 536, loss -3118912.75\n",
      "iter 537, loss -3121512.0\n",
      "iter 538, loss -3124925.25\n",
      "iter 539, loss -3131091.0\n",
      "iter 540, loss -3132241.5\n",
      "iter 541, loss -3134891.5\n",
      "iter 542, loss -3138223.5\n",
      "iter 543, loss -3143206.5\n",
      "iter 544, loss -3148217.5\n",
      "iter 545, loss -3150852.0\n",
      "iter 546, loss -3153066.5\n",
      "iter 547, loss -3157151.25\n",
      "iter 548, loss -3160460.25\n",
      "iter 549, loss -3165518.5\n",
      "iter 550, loss -3167303.5\n",
      "iter 551, loss -3169075.75\n",
      "iter 552, loss -3170508.75\n",
      "iter 553, loss -3175438.25\n",
      "iter 554, loss -3178711.75\n",
      "iter 555, loss -3186316.0\n",
      "iter 556, loss -3189545.0\n",
      "iter 557, loss -3198614.5\n",
      "iter 558, loss -3200742.25\n",
      "iter 559, loss -3202473.75\n",
      "iter 560, loss -3204619.5\n",
      "iter 561, loss -3207312.5\n",
      "iter 562, loss -3209077.0\n",
      "iter 563, loss -3214049.0\n",
      "iter 564, loss -3218083.0\n",
      "iter 565, loss -3220804.5\n",
      "iter 566, loss -3223524.0\n",
      "iter 567, loss -3224981.0\n",
      "iter 568, loss -3228984.75\n",
      "iter 569, loss -3231668.0\n",
      "iter 570, loss -3234894.25\n",
      "iter 571, loss -3237073.75\n",
      "iter 572, loss -3238478.0\n",
      "iter 573, loss -3242576.25\n",
      "iter 574, loss -3246594.0\n",
      "iter 575, loss -3251533.0\n",
      "iter 576, loss -3255498.5\n",
      "iter 577, loss -3257225.5\n",
      "iter 578, loss -3263439.5\n",
      "iter 579, loss -3270789.5\n",
      "iter 580, loss -3274705.5\n",
      "iter 581, loss -3280866.0\n",
      "iter 582, loss -3284140.75\n",
      "iter 583, loss -3286333.5\n",
      "iter 584, loss -3288960.5\n",
      "iter 585, loss -3292295.5\n",
      "iter 586, loss -3295567.0\n",
      "iter 587, loss -3297339.0\n",
      "iter 588, loss -3299999.0\n",
      "iter 589, loss -3304154.0\n",
      "iter 590, loss -3306846.5\n",
      "iter 591, loss -3308979.25\n",
      "iter 592, loss -3315059.25\n",
      "iter 593, loss -3319065.0\n",
      "iter 594, loss -3320502.25\n",
      "iter 595, loss -3323129.0\n",
      "iter 596, loss -3323891.0\n",
      "iter 597, loss -3328939.5\n",
      "iter 598, loss -3331141.0\n",
      "iter 599, loss -3333790.25\n",
      "iter 600, loss -3337117.75\n",
      "iter 601, loss -3338542.5\n",
      "iter 602, loss -3341775.0\n",
      "iter 603, loss -3343911.0\n",
      "iter 604, loss -3347182.0\n",
      "iter 605, loss -3348358.5\n",
      "iter 606, loss -3350530.0\n",
      "iter 607, loss -3352275.25\n",
      "iter 608, loss -3354971.25\n",
      "iter 609, loss -3356121.5\n",
      "iter 610, loss -3357566.0\n",
      "iter 611, loss -3359761.0\n",
      "iter 612, loss -3364799.75\n",
      "iter 613, loss -3366924.5\n",
      "iter 614, loss -3368659.0\n",
      "iter 615, loss -3370784.0\n",
      "iter 616, loss -3376849.0\n",
      "iter 617, loss -3379492.25\n",
      "iter 618, loss -3384489.5\n",
      "iter 619, loss -3387106.0\n",
      "iter 620, loss -3393307.5\n",
      "iter 621, loss -3396019.5\n",
      "iter 622, loss -3397187.0\n",
      "iter 623, loss -3398943.5\n",
      "iter 624, loss -3403031.0\n",
      "iter 625, loss -3403960.0\n",
      "iter 626, loss -3406151.5\n",
      "iter 627, loss -3408299.5\n",
      "iter 628, loss -3410478.0\n",
      "iter 629, loss -3412231.0\n",
      "iter 630, loss -3417265.0\n",
      "iter 631, loss -3419052.0\n",
      "iter 632, loss -3420226.75\n",
      "iter 633, loss -3424390.5\n",
      "iter 634, loss -3425563.25\n",
      "iter 635, loss -3427720.5\n",
      "iter 636, loss -3429437.5\n",
      "iter 637, loss -3434342.25\n",
      "iter 638, loss -3435285.5\n",
      "iter 639, loss -3436671.25\n",
      "iter 640, loss -3440035.25\n",
      "iter 641, loss -3442150.5\n",
      "iter 642, loss -3443275.25\n",
      "iter 643, loss -3444717.5\n",
      "iter 644, loss -3445482.5\n",
      "iter 645, loss -3451608.5\n",
      "iter 646, loss -3453401.0\n",
      "iter 647, loss -3455119.0\n",
      "iter 648, loss -3456552.5\n",
      "iter 649, loss -3459331.25\n",
      "iter 650, loss -3461427.5\n",
      "iter 651, loss -3464053.5\n",
      "iter 652, loss -3465788.75\n",
      "iter 653, loss -3469093.0\n",
      "iter 654, loss -3470868.5\n",
      "iter 655, loss -3475834.25\n",
      "iter 656, loss -3481958.0\n",
      "iter 657, loss -3487895.25\n",
      "iter 658, loss -3491271.0\n",
      "iter 659, loss -3493044.0\n",
      "iter 660, loss -3493816.5\n",
      "iter 661, loss -3498831.5\n",
      "iter 662, loss -3500025.5\n",
      "iter 663, loss -3501160.0\n",
      "iter 664, loss -3505163.75\n",
      "iter 665, loss -3507344.5\n",
      "iter 666, loss -3508801.0\n",
      "iter 667, loss -3513719.0\n",
      "iter 668, loss -3517734.5\n",
      "iter 669, loss -3518906.0\n",
      "iter 670, loss -3522141.75\n",
      "iter 671, loss -3525381.75\n",
      "iter 672, loss -3529571.25\n",
      "iter 673, loss -3531293.5\n",
      "iter 674, loss -3535384.5\n",
      "iter 675, loss -3536009.5\n",
      "iter 676, loss -3538254.0\n",
      "iter 677, loss -3541503.5\n",
      "iter 678, loss -3542676.0\n",
      "iter 679, loss -3543603.25\n",
      "iter 680, loss -3545343.0\n",
      "iter 681, loss -3546493.5\n",
      "iter 682, loss -3550542.0\n",
      "iter 683, loss -3552743.25\n",
      "iter 684, loss -3555369.25\n",
      "iter 685, loss -3556320.75\n",
      "iter 686, loss -3558503.0\n",
      "iter 687, loss -3561764.0\n",
      "iter 688, loss -3562263.0\n",
      "iter 689, loss -3564411.5\n",
      "iter 690, loss -3567717.25\n",
      "iter 691, loss -3569880.25\n",
      "iter 692, loss -3573189.75\n",
      "iter 693, loss -3576557.5\n",
      "iter 694, loss -3579229.25\n",
      "iter 695, loss -3580167.75\n",
      "iter 696, loss -3584228.0\n",
      "iter 697, loss -3589204.75\n",
      "iter 698, loss -3591846.0\n",
      "iter 699, loss -3594531.75\n",
      "iter 700, loss -3597851.0\n",
      "iter 701, loss -3600594.5\n",
      "iter 702, loss -3603212.75\n",
      "iter 703, loss -3605901.5\n",
      "iter 704, loss -3607644.0\n",
      "iter 705, loss -3612630.0\n",
      "iter 706, loss -3616023.75\n",
      "iter 707, loss -3622144.75\n",
      "iter 708, loss -3627181.5\n",
      "iter 709, loss -3629839.5\n",
      "iter 710, loss -3633125.0\n",
      "iter 711, loss -3637155.75\n",
      "iter 712, loss -3638896.0\n",
      "iter 713, loss -3640049.0\n",
      "iter 714, loss -3641437.5\n",
      "iter 715, loss -3644126.0\n",
      "iter 716, loss -3646243.0\n",
      "iter 717, loss -3647680.5\n",
      "iter 718, loss -3653636.0\n",
      "iter 719, loss -3656389.5\n",
      "iter 720, loss -3658589.75\n",
      "iter 721, loss -3659774.5\n",
      "iter 722, loss -3663862.0\n",
      "iter 723, loss -3667953.0\n",
      "iter 724, loss -3671167.75\n",
      "iter 725, loss -3673356.25\n",
      "iter 726, loss -3675185.5\n",
      "iter 727, loss -3675966.0\n",
      "iter 728, loss -3677427.75\n",
      "iter 729, loss -3678849.5\n",
      "iter 730, loss -3680596.5\n",
      "iter 731, loss -3682006.5\n",
      "iter 732, loss -3686825.0\n",
      "iter 733, loss -3687447.5\n",
      "iter 734, loss -3691495.25\n",
      "iter 735, loss -3692431.0\n",
      "iter 736, loss -3693862.0\n",
      "iter 737, loss -3695295.5\n",
      "iter 738, loss -3697914.5\n",
      "iter 739, loss -3700054.5\n",
      "iter 740, loss -3701484.0\n",
      "iter 741, loss -3705557.25\n",
      "iter 742, loss -3707258.0\n",
      "iter 743, loss -3709920.75\n",
      "iter 744, loss -3714848.5\n",
      "iter 745, loss -3716616.25\n",
      "iter 746, loss -3718030.0\n",
      "iter 747, loss -3721299.5\n",
      "iter 748, loss -3722262.5\n",
      "iter 749, loss -3723681.75\n",
      "iter 750, loss -3724300.5\n",
      "iter 751, loss -3726068.0\n",
      "iter 752, loss -3726821.75\n",
      "iter 753, loss -3727980.25\n",
      "iter 754, loss -3731241.0\n",
      "iter 755, loss -3733386.5\n",
      "iter 756, loss -3733888.0\n",
      "iter 757, loss -3735330.25\n",
      "iter 758, loss -3736280.0\n",
      "iter 759, loss -3738465.75\n",
      "iter 760, loss -3740193.0\n",
      "iter 761, loss -3741960.5\n",
      "iter 762, loss -3743119.0\n",
      "iter 763, loss -3744276.5\n",
      "iter 764, loss -3746940.25\n",
      "iter 765, loss -3748707.75\n",
      "iter 766, loss -3752752.5\n",
      "iter 767, loss -3755391.25\n",
      "iter 768, loss -3758662.25\n",
      "iter 769, loss -3760829.5\n",
      "iter 770, loss -3762223.0\n",
      "iter 771, loss -3763171.0\n",
      "iter 772, loss -3764087.5\n",
      "iter 773, loss -3765254.0\n",
      "iter 774, loss -3767895.75\n",
      "iter 775, loss -3772703.0\n",
      "iter 776, loss -3773870.75\n",
      "iter 777, loss -3777180.5\n",
      "iter 778, loss -3779840.5\n",
      "iter 779, loss -3780978.5\n",
      "iter 780, loss -3782759.5\n",
      "iter 781, loss -3786832.25\n",
      "iter 782, loss -3790009.75\n",
      "iter 783, loss -3794028.5\n",
      "iter 784, loss -3795805.5\n",
      "iter 785, loss -3799711.75\n",
      "iter 786, loss -3801868.75\n",
      "iter 787, loss -3802813.25\n",
      "iter 788, loss -3803568.0\n",
      "iter 789, loss -3806205.5\n",
      "iter 790, loss -3807391.5\n",
      "iter 791, loss -3811380.5\n",
      "iter 792, loss -3812808.0\n",
      "iter 793, loss -3814918.5\n",
      "iter 794, loss -3816325.75\n",
      "iter 795, loss -3820451.0\n",
      "iter 796, loss -3821221.0\n",
      "iter 797, loss -3822660.75\n",
      "iter 798, loss -3824098.5\n",
      "iter 799, loss -3827406.0\n",
      "iter 800, loss -3829145.0\n",
      "iter 801, loss -3830580.0\n",
      "iter 802, loss -3831738.0\n",
      "iter 803, loss -3835028.0\n",
      "iter 804, loss -3836447.0\n",
      "iter 805, loss -3841391.5\n",
      "iter 806, loss -3842357.75\n",
      "iter 807, loss -3844084.5\n",
      "iter 808, loss -3845517.0\n",
      "iter 809, loss -3847673.0\n",
      "iter 810, loss -3850970.75\n",
      "iter 811, loss -3852720.5\n",
      "iter 812, loss -3853355.0\n",
      "iter 813, loss -3855469.0\n",
      "iter 814, loss -3856909.5\n",
      "iter 815, loss -3863099.5\n",
      "iter 816, loss -3868082.0\n",
      "iter 817, loss -3868840.0\n",
      "iter 818, loss -3871512.0\n",
      "iter 819, loss -3872909.0\n",
      "iter 820, loss -3873843.5\n",
      "iter 821, loss -3875040.5\n",
      "iter 822, loss -3878378.25\n",
      "iter 823, loss -3881117.0\n",
      "iter 824, loss -3884418.5\n",
      "iter 825, loss -3886603.5\n",
      "iter 826, loss -3888765.5\n",
      "iter 827, loss -3889906.0\n",
      "iter 828, loss -3893890.5\n",
      "iter 829, loss -3896596.75\n",
      "iter 830, loss -3899248.0\n",
      "iter 831, loss -3901034.0\n",
      "iter 832, loss -3901663.0\n",
      "iter 833, loss -3902831.0\n",
      "iter 834, loss -3910514.0\n",
      "iter 835, loss -3911667.0\n",
      "iter 836, loss -3913814.75\n",
      "iter 837, loss -3917724.5\n",
      "iter 838, loss -3919151.0\n",
      "iter 839, loss -3921884.5\n",
      "iter 840, loss -3923336.0\n",
      "iter 841, loss -3924474.0\n",
      "iter 842, loss -3926624.5\n",
      "iter 843, loss -3931602.0\n",
      "iter 844, loss -3932221.5\n",
      "iter 845, loss -3932634.0\n",
      "iter 846, loss -3933411.0\n",
      "iter 847, loss -3934387.0\n",
      "iter 848, loss -3938474.0\n",
      "iter 849, loss -3940231.25\n",
      "iter 850, loss -3941954.0\n",
      "iter 851, loss -3943703.25\n",
      "iter 852, loss -3944452.75\n",
      "iter 853, loss -3946194.0\n",
      "iter 854, loss -3950217.5\n",
      "iter 855, loss -3951930.75\n",
      "iter 856, loss -3952547.5\n",
      "iter 857, loss -3955780.5\n",
      "iter 858, loss -3957192.5\n",
      "iter 859, loss -3958972.25\n",
      "iter 860, loss -3960145.0\n",
      "iter 861, loss -3962350.0\n",
      "iter 862, loss -3962964.75\n",
      "iter 863, loss -3967026.0\n",
      "iter 864, loss -3971075.0\n",
      "iter 865, loss -3971875.5\n",
      "iter 866, loss -3972821.75\n",
      "iter 867, loss -3973756.25\n",
      "iter 868, loss -3975911.5\n",
      "iter 869, loss -3977668.0\n",
      "iter 870, loss -3979823.5\n",
      "iter 871, loss -3980582.25\n",
      "iter 872, loss -3981999.0\n",
      "iter 873, loss -3984618.5\n",
      "iter 874, loss -3987307.75\n",
      "iter 875, loss -3988254.25\n",
      "iter 876, loss -3990385.0\n",
      "iter 877, loss -3992558.75\n",
      "iter 878, loss -3993320.75\n",
      "iter 879, loss -3994466.0\n",
      "iter 880, loss -3995239.5\n",
      "iter 881, loss -3996967.5\n",
      "iter 882, loss -4000205.5\n",
      "iter 883, loss -4001162.0\n",
      "iter 884, loss -4002590.75\n",
      "iter 885, loss -4003980.5\n",
      "iter 886, loss -4004603.0\n",
      "iter 887, loss -4004934.0\n",
      "iter 888, loss -4005561.0\n",
      "iter 889, loss -4011792.5\n",
      "iter 890, loss -4015059.75\n",
      "iter 891, loss -4016196.0\n",
      "iter 892, loss -4017146.0\n",
      "iter 893, loss -4018288.5\n",
      "iter 894, loss -4018924.5\n",
      "iter 895, loss -4020308.0\n",
      "iter 896, loss -4021707.75\n",
      "iter 897, loss -4025718.75\n",
      "iter 898, loss -4026663.5\n",
      "iter 899, loss -4028110.0\n",
      "iter 900, loss -4029045.5\n",
      "iter 901, loss -4031263.75\n",
      "iter 902, loss -4033006.5\n",
      "iter 903, loss -4034169.5\n",
      "iter 904, loss -4035331.75\n",
      "iter 905, loss -4036470.0\n",
      "iter 906, loss -4038587.75\n",
      "iter 907, loss -4039518.0\n",
      "iter 908, loss -4041691.0\n",
      "iter 909, loss -4042625.0\n",
      "iter 910, loss -4045904.5\n",
      "iter 911, loss -4049200.5\n",
      "iter 912, loss -4050941.0\n",
      "iter 913, loss -4058547.0\n",
      "iter 914, loss -4059676.25\n",
      "iter 915, loss -4060446.25\n",
      "iter 916, loss -4065413.75\n",
      "iter 917, loss -4066349.0\n",
      "iter 918, loss -4066841.0\n",
      "iter 919, loss -4068022.0\n",
      "iter 920, loss -4069444.25\n",
      "iter 921, loss -4072004.0\n",
      "iter 922, loss -4073781.75\n",
      "iter 923, loss -4076455.5\n",
      "iter 924, loss -4079622.25\n",
      "iter 925, loss -4081012.25\n",
      "iter 926, loss -4084181.75\n",
      "iter 927, loss -4085605.75\n",
      "iter 928, loss -4086543.75\n",
      "iter 929, loss -4088283.75\n",
      "iter 930, loss -4090436.75\n",
      "iter 931, loss -4090943.5\n",
      "iter 932, loss -4091873.75\n",
      "iter 933, loss -4092828.75\n",
      "iter 934, loss -4095391.5\n",
      "iter 935, loss -4096540.5\n",
      "iter 936, loss -4099840.0\n",
      "iter 937, loss -4100812.0\n",
      "iter 938, loss -4101983.0\n",
      "iter 939, loss -4104199.25\n",
      "iter 940, loss -4106890.5\n",
      "iter 941, loss -4108300.5\n",
      "iter 942, loss -4109724.0\n",
      "iter 943, loss -4114761.0\n",
      "iter 944, loss -4115706.0\n",
      "iter 945, loss -4118327.5\n",
      "iter 946, loss -4118825.0\n",
      "iter 947, loss -4121045.0\n",
      "iter 948, loss -4121555.0\n",
      "iter 949, loss -4124202.0\n",
      "iter 950, loss -4125610.0\n",
      "iter 951, loss -4128224.0\n",
      "iter 952, loss -4129169.5\n",
      "iter 953, loss -4129681.25\n",
      "iter 954, loss -4130438.5\n",
      "iter 955, loss -4132241.0\n",
      "iter 956, loss -4134029.75\n",
      "iter 957, loss -4134431.0\n",
      "iter 958, loss -4135374.5\n",
      "iter 959, loss -4137500.5\n",
      "iter 960, loss -4138248.5\n",
      "iter 961, loss -4139012.5\n",
      "iter 962, loss -4142992.0\n",
      "iter 963, loss -4144778.0\n",
      "iter 964, loss -4147352.25\n",
      "iter 965, loss -4148475.0\n",
      "iter 966, loss -4149248.0\n",
      "iter 967, loss -4152557.5\n",
      "iter 968, loss -4153693.5\n",
      "iter 969, loss -4154826.75\n",
      "iter 970, loss -4156935.5\n",
      "iter 971, loss -4159113.0\n",
      "iter 972, loss -4160909.5\n",
      "iter 973, loss -4162061.0\n",
      "iter 974, loss -4162837.5\n",
      "iter 975, loss -4163958.0\n",
      "iter 976, loss -4164611.75\n",
      "iter 977, loss -4166384.75\n",
      "iter 978, loss -4167534.5\n",
      "iter 979, loss -4167946.5\n",
      "iter 980, loss -4169373.25\n",
      "iter 981, loss -4169717.75\n",
      "iter 982, loss -4172399.0\n",
      "iter 983, loss -4173030.5\n",
      "iter 984, loss -4175156.5\n",
      "iter 985, loss -4175779.5\n",
      "iter 986, loss -4177545.5\n",
      "iter 987, loss -4179313.0\n",
      "iter 988, loss -4180726.0\n",
      "iter 989, loss -4181652.25\n",
      "iter 990, loss -4182600.0\n",
      "iter 991, loss -4184680.5\n",
      "iter 992, loss -4185586.0\n",
      "iter 993, loss -4186733.0\n",
      "iter 994, loss -4187488.5\n",
      "iter 995, loss -4188400.5\n",
      "iter 996, loss -4191050.5\n",
      "iter 997, loss -4192796.5\n",
      "iter 998, loss -4194519.0\n",
      "iter 999, loss -4197816.5\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = bcd(x, sigma, b, lambd, a0, rho, eta, 100, r, max_iter, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_krr_env",
   "language": "python",
   "name": "fast_krr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
