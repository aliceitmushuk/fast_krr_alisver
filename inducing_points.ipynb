{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_random_seed, load_data\n",
    "# from src.opts import apply_nys_precond\n",
    "from src.kernels import _get_kernel\n",
    "import time\n",
    "import numpy as np\n",
    "from pykeops.torch import LazyTensor\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_nys_appx(K_sm, K_mm, lambd, n, r, device):\n",
    "    # Calculate sketch\n",
    "    Phi = torch.randn((n, r), device=device) / (n ** 0.5)\n",
    "    Phi = torch.linalg.qr(Phi, mode='reduced')[0]\n",
    "\n",
    "    Y = K_sm.T @ (K_sm @ Phi) \n",
    "    # + lambd * (K_mm @ Phi)\n",
    "\n",
    "    # Calculate shift\n",
    "    # TODO: Modify the shift to improve stability\n",
    "    shift = torch.finfo(Y.dtype).eps\n",
    "    # Y_shifted = Y + n * shift * Phi\n",
    "    Y_shifted = Y + shift * Phi\n",
    "\n",
    "    # Calculate Phi^T * K * Phi (w/ shift) for Cholesky\n",
    "    choleskytarget = torch.mm(Phi.t(), Y_shifted)\n",
    "\n",
    "    try:\n",
    "        # Perform Cholesky decomposition\n",
    "        C = torch.linalg.cholesky(choleskytarget)\n",
    "    except torch.linalg.LinAlgError:\n",
    "        # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "        eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "        shift = shift + torch.abs(torch.min(eigs))\n",
    "        # add shift to eigenvalues\n",
    "        eigs = eigs + torch.abs(torch.min(eigs))\n",
    "        # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T\n",
    "        C = torch.linalg.cholesky(\n",
    "            torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "    B = torch.linalg.solve_triangular(C.t(), Y_shifted, upper=True, left=False)\n",
    "    U, S, _ = torch.linalg.svd(B, full_matrices=False)\n",
    "    S = torch.max(torch.square(S) - shift, torch.tensor(0.0))\n",
    "\n",
    "    return U, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nys_precond(U, S, rho, g):\n",
    "    UTg = U.t() @ g\n",
    "    dir = U @ (UTg / (S + rho)) + 1/rho * (g - U @ UTg)\n",
    "\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L(K_sm, K_mm, lambd, U, S, rho):\n",
    "    n = U.shape[0]\n",
    "    v = torch.randn(n, device=U.device)\n",
    "    v = v / torch.linalg.norm(v)\n",
    "\n",
    "    max_eig = None\n",
    "\n",
    "    for _ in range(10):  # TODO: Make this a parameter or check tolerance instead\n",
    "        v_old = v.clone()\n",
    "\n",
    "        UTv = U.t() @ v\n",
    "        v = U @ (UTv / ((S + rho) ** (0.5))) + 1/(rho ** 0.5) * (v - U @ UTv)\n",
    "\n",
    "        v = K_sm.T @ (K_sm @ v) + lambd * (K_mm @ v)\n",
    "\n",
    "        UTv = U.t() @ v\n",
    "        v = U @ (UTv / ((S + rho) ** (0.5))) + 1/(rho ** 0.5) * (v - U @ UTv)\n",
    "\n",
    "        max_eig = torch.dot(v_old, v)\n",
    "\n",
    "        v = v / torch.linalg.norm(v)\n",
    "\n",
    "    return max_eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_dict(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, task):\n",
    "    K_nmTb = K_nm.T @ b\n",
    "    residual = K_nm.T @ (K_nm @ a) + lambd * (K_mm @ a) - K_nmTb\n",
    "    rel_residual = torch.norm(residual) / torch.norm(K_nmTb)\n",
    "    loss = 1/2 * (torch.dot(a, residual - K_nmTb) + b_norm ** 2)\n",
    "    metrics_dict = {'rel_residual': rel_residual, 'train_loss': loss}\n",
    "\n",
    "    pred = K_tst @ a\n",
    "\n",
    "    test_metric_name = 'test_acc' if task == 'classification' else 'test_mse'\n",
    "    if task == 'classification':\n",
    "        test_metric = torch.sum(torch.sign(pred) == b_tst) / b_tst.shape[0]\n",
    "        metrics_dict[test_metric_name] = test_metric\n",
    "    else:\n",
    "        test_metric = 1/2 * torch.norm(pred - b_tst) ** 2 / b_tst.shape[0]\n",
    "        smape = torch.sum((pred - b_tst).abs() /\n",
    "                          ((pred.abs() + b_tst.abs()) / 2)) / b_tst.shape[0]\n",
    "        metrics_dict[test_metric_name] = test_metric\n",
    "        metrics_dict['smape'] = smape\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_log_metrics(K_nm, K_mm, K_tst, y, b, b_tst, lambd, b_norm, iter_time,\n",
    "                            task, i, log_freq):\n",
    "    iter_time_dict = {'iter_time': iter_time}\n",
    "    if (i + 1) % log_freq == 0:\n",
    "        wandb.log(iter_time_dict |\n",
    "                  compute_metrics_dict(K_nm, K_mm, K_tst, y, b, b_tst, lambd, b_norm, task))\n",
    "    else:\n",
    "        wandb.log(iter_time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketchysgd(x, b, x_tst, b_tst, kernel_params, m, lambd, task, a0, bg, bH, r, rho, max_iter, log_freq, device):\n",
    "    n = x.shape[0]\n",
    "    b_norm = torch.linalg.norm(b)\n",
    "   \n",
    "    start_time = time.time()\n",
    "\n",
    "    inducing_pts = torch.from_numpy(np.random.choice(n, m, replace=False))\n",
    "\n",
    "    # Get inducing points kernel\n",
    "    x_inducing_i = LazyTensor(x[inducing_pts][:, None, :])\n",
    "    x_inducing_j = LazyTensor(x[inducing_pts][None, :, :])\n",
    "    K_mm = _get_kernel(x_inducing_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel between full training set and inducing points\n",
    "    x_i = LazyTensor(x[:, None, :])\n",
    "    K_nm = _get_kernel(x_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel for test set\n",
    "    x_tst_i = LazyTensor(x_tst[:, None, :])\n",
    "    K_tst = _get_kernel(x_tst_i, x_inducing_j, kernel_params)\n",
    "    \n",
    "    # Compute the preconditioner\n",
    "    hess_pts = torch.from_numpy(np.random.choice(n, bH, replace=False))\n",
    "    x_hess_i = LazyTensor(x[hess_pts][:, None, :])\n",
    "    K_sm = _get_kernel(x_hess_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    adj_factor = (n / bH) ** 0.5\n",
    "\n",
    "    U, S = rand_nys_appx(adj_factor * K_sm, K_mm, lambd, m, r, device)\n",
    "\n",
    "    # Automatically compute the learning rate\n",
    "    # Do so as in PROMISE -- matvecs with inverse preconditioner and subsampled Hessian in factorized form\n",
    "    hess_pts_lr = torch.from_numpy(np.random.choice(n, bH, replace=False))\n",
    "    x_hess_lr_i = LazyTensor(x[hess_pts_lr][:, None, :])\n",
    "    K_sm_lr = _get_kernel(x_hess_lr_i, x_inducing_j, kernel_params)\n",
    "    eta = 0.5 / (get_L(adj_factor * K_sm_lr, K_mm, lambd, U, S, rho))\n",
    "\n",
    "    a = a0.clone()\n",
    "    iter_time = time.time() - start_time\n",
    "\n",
    "    # Compute and log metrics before any optimization is performed\n",
    "    compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                        task, -1, log_freq)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Get a stochastic gradient\n",
    "        # TODO: Use a shuffling approach instead of random sampling to match PROMISE\n",
    "        idx = torch.from_numpy(np.random.choice(n, bg, replace=False))\n",
    "        x_idx_i = LazyTensor(x[idx][:, None, :])\n",
    "        K_nm_idx = _get_kernel(x_idx_i, x_inducing_j, kernel_params)\n",
    "        g = n/bg * (K_nm_idx.T @ (K_nm_idx @ a - b[idx])) + lambd * (K_mm @ a)\n",
    "    \n",
    "        # Apply the preconditioner\n",
    "        dir = apply_nys_precond(U, S, rho, g)\n",
    "    \n",
    "        # Update params w/ auto learning rate and preconditioned stochastic gradient\n",
    "        a -= eta * dir\n",
    "    \n",
    "        # Call function to compute and log metrics (as necessary)\n",
    "        iter_time = time.time() - start_time\n",
    "        compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                    task, i, log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketchysvrg(x, b, x_tst, b_tst, kernel_params, m, lambd, task, a0, bg, bH, r, rho, update_freq, max_iter, log_freq, device):\n",
    "    n = x.shape[0]\n",
    "    b_norm = torch.linalg.norm(b)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    inducing_pts = torch.from_numpy(np.random.choice(n, m, replace=False))\n",
    "\n",
    "    # Get inducing points kernel\n",
    "    x_inducing_i = LazyTensor(x[inducing_pts][:, None, :])\n",
    "    x_inducing_j = LazyTensor(x[inducing_pts][None, :, :])\n",
    "    K_mm = _get_kernel(x_inducing_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel between full training set and inducing points\n",
    "    x_i = LazyTensor(x[:, None, :])\n",
    "    K_nm = _get_kernel(x_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel for test set\n",
    "    x_tst_i = LazyTensor(x_tst[:, None, :])\n",
    "    K_tst = _get_kernel(x_tst_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Compute the preconditioner\n",
    "    hess_pts = torch.from_numpy(np.random.choice(n, bH, replace=False))\n",
    "    x_hess_i = LazyTensor(x[hess_pts][:, None, :])\n",
    "    K_sm = _get_kernel(x_hess_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    adj_factor = (n / bH) ** 0.5\n",
    "\n",
    "    U, S = rand_nys_appx(adj_factor * K_sm, K_mm, lambd, m, r, device)\n",
    "\n",
    "    # Automatically compute the learning rate\n",
    "    # Do so as in PROMISE -- matvecs with inverse preconditioner and subsampled Hessian in factorized form\n",
    "    hess_pts_lr = torch.from_numpy(np.random.choice(n, bH, replace=False))\n",
    "    x_hess_lr_i = LazyTensor(x[hess_pts_lr][:, None, :])\n",
    "    K_sm_lr = _get_kernel(x_hess_lr_i, x_inducing_j, kernel_params)\n",
    "    eta = 0.5 / get_L(adj_factor * K_sm_lr, K_mm, lambd, U, S, rho)\n",
    "\n",
    "    a = a0.clone()\n",
    "    a_tilde = None\n",
    "    g_bar = None\n",
    "    iter_time = time.time() - start_time\n",
    "\n",
    "    # Compute and log metrics before any optimization is performed\n",
    "    compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                            task, -1, log_freq)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Update snapshot and full gradient at snapshot\n",
    "        if i % update_freq == 0:\n",
    "            a_tilde = a.clone()\n",
    "            g_bar = K_nm.T @ (K_nm @ a_tilde - b) + lambd * (K_mm @ a_tilde)\n",
    "\n",
    "        # Get a stochastic gradient\n",
    "        # TODO: Use a shuffling approach instead of random sampling to match PROMISE\n",
    "        idx = torch.from_numpy(np.random.choice(n, bg, replace=False))\n",
    "        x_idx_i = LazyTensor(x[idx][:, None, :])\n",
    "        K_nm_idx = _get_kernel(x_idx_i, x_inducing_j, kernel_params)\n",
    "        a_diff = a - a_tilde\n",
    "        g_diff = n/bg * (K_nm_idx.T @ (K_nm_idx @ a_diff)) + lambd * (K_mm @ a_diff)\n",
    "\n",
    "        # Apply the preconditioner\n",
    "        dir = apply_nys_precond(U, S, rho, g_diff + g_bar)\n",
    "        # dir = apply_nys_precond(U, S, torch.linalg.norm(g_diff + g_bar), g_diff + g_bar)\n",
    "\n",
    "        # Update params w/ auto learning rate and preconditioned stochastic gradient\n",
    "        a -= eta * dir\n",
    "        # a -= dir / torch.maximum(torch.dot(dir, g_diff + g_bar), torch.tensor([1], device=device))\n",
    "\n",
    "        # Call function to compute and log metrics (as necessary)\n",
    "        iter_time = time.time() - start_time\n",
    "        compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                                task, i, log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nystrom_pcg(x, b, x_tst, b_tst, kernel_params, m, lambd, task, a0, r, rho, max_iter, log_freq, device):\n",
    "    n = x.shape[0]\n",
    "    b_norm = torch.linalg.norm(b)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    inducing_pts = torch.from_numpy(np.random.choice(n, m, replace=False))\n",
    "\n",
    "    # Get inducing points kernel\n",
    "    x_inducing_i = LazyTensor(x[inducing_pts][:, None, :])\n",
    "    x_inducing_j = LazyTensor(x[inducing_pts][None, :, :])\n",
    "    K_mm = _get_kernel(x_inducing_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel between full training set and inducing points\n",
    "    x_i = LazyTensor(x[:, None, :])\n",
    "    K_nm = _get_kernel(x_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel for test set\n",
    "    x_tst_i = LazyTensor(x_tst[:, None, :])\n",
    "    K_tst = _get_kernel(x_tst_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    b_restricted = K_nm.T @ b\n",
    "\n",
    "    # Compute the preconditioner\n",
    "    U, S = rand_nys_appx(K_nm, K_mm, lambd, m, r, device)\n",
    "\n",
    "    # Initialize PCG\n",
    "    a = a0.clone()\n",
    "\n",
    "    resid = b_restricted - (K_nm.T @ (K_nm @ a0) + lambd * (K_mm @ a0))\n",
    "    z = apply_nys_precond(U, S, rho, resid)\n",
    "    p = z.clone()\n",
    "\n",
    "    iter_time = time.time() - start_time\n",
    "\n",
    "    # Compute and log metrics before any optimization is performed\n",
    "    compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                            task, -1, log_freq)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Perform PCG iteration\n",
    "        v = K_nm.T @ (K_nm @ p) + lambd * (K_mm @ p)\n",
    "        alpha = torch.dot(z, resid) / torch.dot(p, v)\n",
    "        a += alpha * p\n",
    "\n",
    "        rTz = torch.dot(resid, z)\n",
    "        resid -= alpha * v\n",
    "        z = apply_nys_precond(U, S, rho, resid)\n",
    "        beta = torch.dot(resid, z) / rTz\n",
    "\n",
    "        p = z + beta * p\n",
    "\n",
    "        # Call function to compute and log metrics (as necessary)\n",
    "        iter_time = time.time() - start_time\n",
    "        compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                                task, i, log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'homo'\n",
    "seed = 0\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xtst, ytr, ytst = load_data(data, seed, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10000  # Number of inducing points\n",
    "kernel_params = {'type': 'l1_laplace', 'sigma': 5120}\n",
    "lambd = 1e-3\n",
    "task = 'regression'\n",
    "bg = 256\n",
    "r = 10\n",
    "rho = 1e0 # 1e-3\n",
    "update_freq = int(Xtr.shape[0] / bg)\n",
    "max_iter = 3000\n",
    "log_freq = 100\n",
    "\n",
    "# opt = 'sketchysgd'\n",
    "opt = 'sketchysvrg'\n",
    "# opt = 'nystrom_pcg'\n",
    "\n",
    "wandb_project = \"sksgd_refactor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_args = {\n",
    "#     'dataset': data,\n",
    "#     'task': task,\n",
    "#     'kernel_params': kernel_params,\n",
    "#     'lambd': lambd,\n",
    "#     'm': m,\n",
    "#     'opt': opt,\n",
    "#     'bg': bg,\n",
    "#     'r': r,\n",
    "#     'rho': rho,\n",
    "#     'max_iter': max_iter,\n",
    "#     'log_freq': log_freq,\n",
    "#     'seed': seed,\n",
    "#     'device': device\n",
    "# }\n",
    "\n",
    "experiment_args = {\n",
    "    'dataset': data,\n",
    "    'task': task,\n",
    "    'kernel_params': kernel_params,\n",
    "    'lambd': lambd,\n",
    "    'm': m,\n",
    "    'opt': opt,\n",
    "    'r': r,\n",
    "    'rho': rho,\n",
    "    'max_iter': max_iter,\n",
    "    'log_freq': log_freq,\n",
    "    'seed': seed,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "if opt in ['sketchysgd', 'sketchysvrg']:\n",
    "    experiment_args['bg'] = bg\n",
    "if opt == 'sketchysvrg':\n",
    "    experiment_args['update_freq'] = update_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pratikr/fast_krr/wandb/run-20240403_190219-ociav8fy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sketchy-opts/sksgd_refactor/runs/ociav8fy' target=\"_blank\">amber-serenity-1</a></strong> to <a href='https://wandb.ai/sketchy-opts/sksgd_refactor' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sketchy-opts/sksgd_refactor' target=\"_blank\">https://wandb.ai/sketchy-opts/sksgd_refactor</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sketchy-opts/sksgd_refactor/runs/ociav8fy' target=\"_blank\">https://wandb.ai/sketchy-opts/sksgd_refactor/runs/ociav8fy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e293c9f27ec47c6b70e3b8db9d17ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter_time</td><td>▄▆█▃▂▁▅▅▂▃▂▃▇▂▂▆▆█▆▄▄▁▄▃▃▃▄▅▂▅▂▂▄▃▅▂▂▂▂▄</td></tr><tr><td>rel_residual</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>smape</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_mse</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter_time</td><td>0.08839</td></tr><tr><td>rel_residual</td><td>0.00341</td></tr><tr><td>smape</td><td>0.04929</td></tr><tr><td>test_mse</td><td>0.09071</td></tr><tr><td>train_loss</td><td>8913.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-serenity-1</strong> at: <a href='https://wandb.ai/sketchy-opts/sksgd_refactor/runs/ociav8fy' target=\"_blank\">https://wandb.ai/sketchy-opts/sksgd_refactor/runs/ociav8fy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240403_190219-ociav8fy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=wandb_project, config=experiment_args):\n",
    "    # Access the experiment configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    # Load the dataset\n",
    "    Xtr, Xtst, ytr, ytst = load_data(config.dataset, config.seed, config.device)\n",
    "\n",
    "    bH = int(Xtr.shape[0] ** 0.5)\n",
    "\n",
    "    # Initialize at 0\n",
    "    a0 = torch.zeros(config.m, device=config.device)\n",
    "\n",
    "    # Run the optimizer\n",
    "    with torch.no_grad():\n",
    "        if config.opt == 'sketchysgd':\n",
    "            sketchysgd(Xtr, ytr, Xtst, ytst, config.kernel_params, config.m, config.lambd,\n",
    "                    config.task, a0, config.bg, bH, config.r, config.rho, \n",
    "                    config.max_iter, config.log_freq, config.device)\n",
    "        elif config.opt == 'sketchysvrg':\n",
    "            sketchysvrg(Xtr, ytr, Xtst, ytst, config.kernel_params, config.m, config.lambd,\n",
    "                        config.task, a0, config.bg, bH, config.r, config.rho, config.update_freq,\n",
    "                        config.max_iter, config.log_freq, config.device)\n",
    "        elif config.opt == 'nystrom_pcg':\n",
    "            nystrom_pcg(Xtr, ytr, Xtst, ytst, config.kernel_params, config.m, config.lambd,\n",
    "                        config.task, a0, config.r, config.rho, config.max_iter, \n",
    "                        config.log_freq, config.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_krr_env",
   "language": "python",
   "name": "fast_krr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
