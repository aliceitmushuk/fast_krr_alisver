{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_random_seed, load_data\n",
    "from src.opts import apply_nys_precond\n",
    "from src.kernels import get_kernel\n",
    "import time\n",
    "import numpy as np\n",
    "from pykeops.torch import LazyTensor\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_nys_appx(K_sm, K_mm, lambd, n, r, device):\n",
    "    # Calculate sketch\n",
    "    Phi = torch.randn((n, r), device=device) / (n ** 0.5)\n",
    "    Phi = torch.linalg.qr(Phi, mode='reduced')[0]\n",
    "\n",
    "    Y = K_sm.T @ (K_sm @ Phi) \n",
    "    # + lambd * (K_mm @ Phi)\n",
    "\n",
    "    # Calculate shift\n",
    "    # TODO: Modify the shift to improve stability\n",
    "    shift = torch.finfo(Y.dtype).eps\n",
    "    Y_shifted = Y + n * shift * Phi\n",
    "\n",
    "    # Calculate Phi^T * K * Phi (w/ shift) for Cholesky\n",
    "    choleskytarget = torch.mm(Phi.t(), Y_shifted)\n",
    "\n",
    "    try:\n",
    "        # Perform Cholesky decomposition\n",
    "        C = torch.linalg.cholesky(choleskytarget)\n",
    "    except torch.linalg.LinAlgError:\n",
    "        # eigendecomposition, eigenvalues and eigenvector matrix\n",
    "        eigs, eigvectors = torch.linalg.eigh(choleskytarget)\n",
    "        shift = shift + torch.abs(torch.min(eigs))\n",
    "        # add shift to eigenvalues\n",
    "        eigs = eigs + torch.abs(torch.min(eigs))\n",
    "        # put back the matrix for Cholesky by eigenvector * eigenvalues after shift * eigenvector^T\n",
    "        C = torch.linalg.cholesky(\n",
    "            torch.mm(eigvectors, torch.mm(torch.diag(eigs), eigvectors.T)))\n",
    "\n",
    "    B = torch.linalg.solve_triangular(C.t(), Y_shifted, upper=True, left=False)\n",
    "    U, S, _ = torch.linalg.svd(B, full_matrices=False)\n",
    "    S = torch.max(torch.square(S) - shift, torch.tensor(0.0))\n",
    "\n",
    "    return U, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L(K_sm, K_mm, lambd, U, S, rho):\n",
    "    n = U.shape[0]\n",
    "    v = torch.randn(n, device=U.device)\n",
    "    v = v / torch.linalg.norm(v)\n",
    "\n",
    "    max_eig = None\n",
    "\n",
    "    for _ in range(10):  # TODO: Make this a parameter or check tolerance instead\n",
    "        v_old = v.clone()\n",
    "\n",
    "        UTv = U.t() @ v\n",
    "        v = U @ (UTv / ((S + rho) ** (0.5))) + 1/(rho ** 0.5) * (v - U @ UTv)\n",
    "\n",
    "        v = K_sm.T @ (K_sm @ v) + lambd * (K_mm @ v)\n",
    "\n",
    "        UTv = U.t() @ v\n",
    "        v = U @ (UTv / ((S + rho) ** (0.5))) + 1/(rho ** 0.5) * (v - U @ UTv)\n",
    "\n",
    "        max_eig = torch.dot(v_old, v)\n",
    "\n",
    "        v = v / torch.linalg.norm(v)\n",
    "\n",
    "    return max_eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_dict(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, task):\n",
    "    K_nmTb = K_nm.T @ b\n",
    "    residual = K_nm.T @ (K_nm @ a) + lambd * (K_mm @ a) - K_nmTb\n",
    "    rel_residual = torch.norm(residual) / torch.norm(K_nmTb)\n",
    "    loss = 1/2 * (torch.dot(a, residual - K_nmTb) + b_norm ** 2)\n",
    "    metrics_dict = {'rel_residual': rel_residual, 'train_loss': loss}\n",
    "\n",
    "    pred = K_tst @ a\n",
    "\n",
    "    test_metric_name = 'test_acc' if task == 'classification' else 'test_mse'\n",
    "    if task == 'classification':\n",
    "        test_metric = torch.sum(torch.sign(pred) == b_tst) / b_tst.shape[0]\n",
    "        metrics_dict[test_metric_name] = test_metric\n",
    "    else:\n",
    "        test_metric = 1/2 * torch.norm(pred - b_tst) ** 2 / b_tst.shape[0]\n",
    "        smape = torch.sum((pred - b_tst).abs() /\n",
    "                          ((pred.abs() + b_tst.abs()) / 2)) / b_tst.shape[0]\n",
    "        metrics_dict[test_metric_name] = test_metric\n",
    "        metrics_dict['smape'] = smape\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_log_metrics(K_nm, K_mm, K_tst, y, b, b_tst, lambd, b_norm, iter_time,\n",
    "                            task, i, log_freq):\n",
    "    iter_time_dict = {'iter_time': iter_time}\n",
    "    if (i + 1) % log_freq == 0:\n",
    "        wandb.log(iter_time_dict |\n",
    "                  compute_metrics_dict(K_nm, K_mm, K_tst, y, b, b_tst, lambd, b_norm, task))\n",
    "    else:\n",
    "        wandb.log(iter_time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketchysgd(x, b, x_tst, b_tst, kernel_params, m, lambd, task, a0, bg, bH, r, rho, max_iter, log_freq, device):\n",
    "    n = x.shape[0]\n",
    "    b_norm = torch.linalg.norm(b)\n",
    "   \n",
    "    start_time = time.time()\n",
    "\n",
    "    inducing_pts = torch.from_numpy(np.random.choice(n, m, replace=False))\n",
    "\n",
    "    # Get inducing points kernel\n",
    "    x_inducing_i = LazyTensor(x[inducing_pts][:, None, :])\n",
    "    x_inducing_j = LazyTensor(x[inducing_pts][None, :, :])\n",
    "    K_mm = get_kernel(x_inducing_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel between full training set and inducing points\n",
    "    x_i = LazyTensor(x[:, None, :])\n",
    "    K_nm = get_kernel(x_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel for test set\n",
    "    x_tst_i = LazyTensor(x_tst[:, None, :])\n",
    "    K_tst = get_kernel(x_tst_i, x_inducing_j, kernel_params)\n",
    "    \n",
    "    # Compute the preconditioner\n",
    "    hess_pts = torch.from_numpy(np.random.choice(n, bH, replace=False))\n",
    "    x_hess_i = LazyTensor(x[hess_pts][:, None, :])\n",
    "    K_sm = get_kernel(x_hess_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    adj_factor = (n / bH) ** 0.5\n",
    "\n",
    "    U, S = rand_nys_appx(adj_factor * K_sm, K_mm, lambd, m, r, device)\n",
    "\n",
    "    # Automatically compute the learning rate\n",
    "    # Do so as in PROMISE -- matvecs with inverse preconditioner and subsampled Hessian in factorized form\n",
    "    hess_pts_lr = torch.from_numpy(np.random.choice(n, bH, replace=False))\n",
    "    x_hess_lr_i = LazyTensor(x[hess_pts_lr][:, None, :])\n",
    "    K_sm_lr = get_kernel(x_hess_lr_i, x_inducing_j, kernel_params)\n",
    "    eta = 0.5 / (get_L(adj_factor * K_sm_lr, K_mm, lambd, U, S, rho))\n",
    "\n",
    "    a = a0.clone()\n",
    "    iter_time = time.time() - start_time\n",
    "\n",
    "    # Compute and log metrics before any optimization is performed\n",
    "    compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                        task, -1, log_freq)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Get a stochastic gradient\n",
    "        # TODO: Use a shuffling approach instead of random sampling to match PROMISE\n",
    "        idx = torch.from_numpy(np.random.choice(n, bg, replace=False))\n",
    "        x_idx_i = LazyTensor(x[idx][:, None, :])\n",
    "        K_nm_idx = get_kernel(x_idx_i, x_inducing_j, kernel_params)\n",
    "        g = n/bg * (K_nm_idx.T @ (K_nm_idx @ a - b[idx])) + lambd * (K_mm @ a)\n",
    "    \n",
    "        # Apply the preconditioner\n",
    "        dir = apply_nys_precond(U, S, rho, g)\n",
    "    \n",
    "        # Update params w/ auto learning rate and preconditioned stochastic gradient\n",
    "        a -= eta * dir\n",
    "    \n",
    "        # Call function to compute and log metrics (as necessary)\n",
    "        iter_time = time.time() - start_time\n",
    "        compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                    task, i, log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sketchysvrg(x, b, x_tst, b_tst, kernel_params, m, lambd, task, a0, bg, bH, r, rho, update_freq, max_iter, log_freq, device):\n",
    "    n = x.shape[0]\n",
    "    b_norm = torch.linalg.norm(b)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    inducing_pts = torch.from_numpy(np.random.choice(n, m, replace=False))\n",
    "\n",
    "    # Get inducing points kernel\n",
    "    x_inducing_i = LazyTensor(x[inducing_pts][:, None, :])\n",
    "    x_inducing_j = LazyTensor(x[inducing_pts][None, :, :])\n",
    "    K_mm = get_kernel(x_inducing_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel between full training set and inducing points\n",
    "    x_i = LazyTensor(x[:, None, :])\n",
    "    K_nm = get_kernel(x_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel for test set\n",
    "    x_tst_i = LazyTensor(x_tst[:, None, :])\n",
    "    K_tst = get_kernel(x_tst_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Compute the preconditioner\n",
    "    hess_pts = torch.from_numpy(np.random.choice(n, bH, replace=False))\n",
    "    x_hess_i = LazyTensor(x[hess_pts][:, None, :])\n",
    "    K_sm = get_kernel(x_hess_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    adj_factor = (n / bH) ** 0.5\n",
    "\n",
    "    U, S = rand_nys_appx(adj_factor * K_sm, K_mm, lambd, m, r, device)\n",
    "\n",
    "    # Automatically compute the learning rate\n",
    "    # Do so as in PROMISE -- matvecs with inverse preconditioner and subsampled Hessian in factorized form\n",
    "    hess_pts_lr = torch.from_numpy(np.random.choice(n, bH, replace=False))\n",
    "    x_hess_lr_i = LazyTensor(x[hess_pts_lr][:, None, :])\n",
    "    K_sm_lr = get_kernel(x_hess_lr_i, x_inducing_j, kernel_params)\n",
    "    eta = (1/10) / get_L(adj_factor * K_sm_lr, K_mm, lambd, U, S, rho)\n",
    "\n",
    "    a = a0.clone()\n",
    "    a_tilde = None\n",
    "    g_bar = None\n",
    "    iter_time = time.time() - start_time\n",
    "\n",
    "    # Compute and log metrics before any optimization is performed\n",
    "    compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                            task, -1, log_freq)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Update snapshot and full gradient at snapshot\n",
    "        if i % update_freq == 0:\n",
    "            a_tilde = a.clone()\n",
    "            g_bar = K_nm.T @ (K_nm @ a_tilde - b) + lambd * (K_mm @ a_tilde)\n",
    "\n",
    "        # Get a stochastic gradient\n",
    "        # TODO: Use a shuffling approach instead of random sampling to match PROMISE\n",
    "        idx = torch.from_numpy(np.random.choice(n, bg, replace=False))\n",
    "        x_idx_i = LazyTensor(x[idx][:, None, :])\n",
    "        K_nm_idx = get_kernel(x_idx_i, x_inducing_j, kernel_params)\n",
    "        a_diff = a - a_tilde\n",
    "        g_diff = n/bg * (K_nm_idx.T @ (K_nm_idx @ a_diff)) + lambd * (K_mm @ a_diff)\n",
    "\n",
    "        # Apply the preconditioner\n",
    "        dir = apply_nys_precond(U, S, rho, g_diff + g_bar)\n",
    "\n",
    "        # Update params w/ auto learning rate and preconditioned stochastic gradient\n",
    "        a -= eta * dir\n",
    "\n",
    "        # Call function to compute and log metrics (as necessary)\n",
    "        iter_time = time.time() - start_time\n",
    "        compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                                task, i, log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nystrom_pcg(x, b, x_tst, b_tst, kernel_params, m, lambd, task, a0, r, rho, max_iter, log_freq, device):\n",
    "    n = x.shape[0]\n",
    "    b_norm = torch.linalg.norm(b)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    inducing_pts = torch.from_numpy(np.random.choice(n, m, replace=False))\n",
    "\n",
    "    # Get inducing points kernel\n",
    "    x_inducing_i = LazyTensor(x[inducing_pts][:, None, :])\n",
    "    x_inducing_j = LazyTensor(x[inducing_pts][None, :, :])\n",
    "    K_mm = get_kernel(x_inducing_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel between full training set and inducing points\n",
    "    x_i = LazyTensor(x[:, None, :])\n",
    "    K_nm = get_kernel(x_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    # Get kernel for test set\n",
    "    x_tst_i = LazyTensor(x_tst[:, None, :])\n",
    "    K_tst = get_kernel(x_tst_i, x_inducing_j, kernel_params)\n",
    "\n",
    "    b_restricted = K_nm.T @ b\n",
    "\n",
    "    # Compute the preconditioner\n",
    "    U, S = rand_nys_appx(K_nm, K_mm, lambd, m, r, device)\n",
    "\n",
    "    # Initialize PCG\n",
    "    a = a0.clone()\n",
    "\n",
    "    resid = b_restricted - (K_nm.T @ (K_nm @ a0) + lambd * (K_mm @ a0))\n",
    "    z = apply_nys_precond(U, S, rho, resid)\n",
    "    p = z.clone()\n",
    "\n",
    "    iter_time = time.time() - start_time\n",
    "\n",
    "    # Compute and log metrics before any optimization is performed\n",
    "    compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                            task, -1, log_freq)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Perform PCG iteration\n",
    "        v = K_nm.T @ (K_nm @ p) + lambd * (K_mm @ p)\n",
    "        alpha = torch.dot(z, resid) / torch.dot(p, v)\n",
    "        a += alpha * p\n",
    "\n",
    "        rTz = torch.dot(resid, z)\n",
    "        resid -= alpha * v\n",
    "        z = apply_nys_precond(U, S, rho, resid)\n",
    "        beta = torch.dot(resid, z) / rTz\n",
    "\n",
    "        p = z + beta * p\n",
    "\n",
    "        # Call function to compute and log metrics (as necessary)\n",
    "        iter_time = time.time() - start_time\n",
    "        compute_and_log_metrics(K_nm, K_mm, K_tst, a, b, b_tst, lambd, b_norm, iter_time,\n",
    "                                task, i, log_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'homo'\n",
    "seed = 0\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xtst, ytr, ytst = load_data(data, seed, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10000  # Number of inducing points\n",
    "kernel_params = {'type': 'l1_laplace', 'sigma': 5120}\n",
    "lambd = 1e-3\n",
    "task = 'regression'\n",
    "bg = 256\n",
    "r = 30\n",
    "rho = 1e-1 # 1e-3\n",
    "update_freq = int(Xtr.shape[0] / bg)\n",
    "max_iter = 10000\n",
    "log_freq = 100\n",
    "\n",
    "# opt = 'sketchysgd'\n",
    "opt = 'sketchysvrg'\n",
    "# opt = 'nystrom_pcg'\n",
    "\n",
    "wandb_project = \"sksgd_krr_testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_args = {\n",
    "#     'dataset': data,\n",
    "#     'task': task,\n",
    "#     'kernel_params': kernel_params,\n",
    "#     'lambd': lambd,\n",
    "#     'm': m,\n",
    "#     'opt': opt,\n",
    "#     'bg': bg,\n",
    "#     'r': r,\n",
    "#     'rho': rho,\n",
    "#     'max_iter': max_iter,\n",
    "#     'log_freq': log_freq,\n",
    "#     'seed': seed,\n",
    "#     'device': device\n",
    "# }\n",
    "\n",
    "experiment_args = {\n",
    "    'dataset': data,\n",
    "    'task': task,\n",
    "    'kernel_params': kernel_params,\n",
    "    'lambd': lambd,\n",
    "    'm': m,\n",
    "    'opt': opt,\n",
    "    'r': r,\n",
    "    'rho': rho,\n",
    "    'max_iter': max_iter,\n",
    "    'log_freq': log_freq,\n",
    "    'seed': seed,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "if opt in ['sketchysgd', 'sketchysvrg']:\n",
    "    experiment_args['bg'] = bg\n",
    "if opt == 'sketchysvrg':\n",
    "    experiment_args['update_freq'] = update_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpratikrathore8\u001b[0m (\u001b[33msketchy-opts\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pratikr/fast_krr/wandb/run-20240320_175019-ucasgj3h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sketchy-opts/sksgd_krr_testing/runs/ucasgj3h' target=\"_blank\">deft-valley-28</a></strong> to <a href='https://wandb.ai/sketchy-opts/sksgd_krr_testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sketchy-opts/sksgd_krr_testing' target=\"_blank\">https://wandb.ai/sketchy-opts/sksgd_krr_testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sketchy-opts/sksgd_krr_testing/runs/ucasgj3h' target=\"_blank\">https://wandb.ai/sketchy-opts/sksgd_krr_testing/runs/ucasgj3h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps] Warning : at least one of the input tensors is not contiguous. Consider using contiguous data arrays to avoid unnecessary copies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3988738/3064188348.py\", line 20, in <module>\n",
      "    sketchysvrg(Xtr, ytr, Xtst, ytst, config.kernel_params, config.m, config.lambd,\n",
      "  File \"/tmp/ipykernel_3988738/3145367713.py\", line 61, in sketchysvrg\n",
      "    g_diff = n/bg * (K_nm_idx.T @ (K_nm_idx @ a_diff)) + lambd * (K_mm @ a_diff)\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/lazy_tensor.py\", line 2524, in __matmul__\n",
      "    Kv = Kv.sum(Kv.dim() - 2, **kwargs)  # Matrix-vector or Matrix-matrix product\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/lazy_tensor.py\", line 2096, in sum\n",
      "    return self.reduction(\"Sum\", axis=axis, **kwargs)\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/lazy_tensor.py\", line 775, in reduction\n",
      "    return res()\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/lazy_tensor.py\", line 957, in __call__\n",
      "    return self.callfun(*args, *self.variables, **self.kwargs)\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/torch/generic/generic_red.py\", line 687, in __call__\n",
      "    out = GenredAutograd_fun(params, *args)\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/torch/generic/generic_red.py\", line 383, in GenredAutograd_fun\n",
      "    return GenredAutograd.apply(*inputs)[0]\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/torch/autograd/function.py\", line 553, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/torch/generic/generic_red.py\", line 291, in forward\n",
      "    return GenredAutograd_base._forward(*inputs)\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/torch/generic/generic_red.py\", line 121, in _forward\n",
      "    result = myconv.genred_pytorch(\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/keops_io/LoadKeOps.py\", line 236, in genred\n",
      "    self.call_keops(nx, ny)\n",
      "  File \"/home/pratikr/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/keops_io/LoadKeOps_nvrtc.py\", line 48, in call_keops\n",
      "    self.launch_keops(\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be2aa4da12b4d6ea6e1b89a3bc445c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter_time</td><td>▄▄▃▂█▂▃▄▃▃▂▄▂▆▃▂▃▂▂▃▂▁▂▃▂▃▃▁▁▃▂▂▂▂▂▂▂▂▆▁</td></tr><tr><td>rel_residual</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>smape</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_mse</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter_time</td><td>0.0885</td></tr><tr><td>rel_residual</td><td>0.00015</td></tr><tr><td>smape</td><td>0.05496</td></tr><tr><td>test_mse</td><td>0.11426</td></tr><tr><td>train_loss</td><td>11356.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-valley-28</strong> at: <a href='https://wandb.ai/sketchy-opts/sksgd_krr_testing/runs/ucasgj3h' target=\"_blank\">https://wandb.ai/sketchy-opts/sksgd_krr_testing/runs/ucasgj3h</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240320_175019-ucasgj3h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     sketchysgd(Xtr, ytr, Xtst, ytst, config\u001b[38;5;241m.\u001b[39mkernel_params, config\u001b[38;5;241m.\u001b[39mm, config\u001b[38;5;241m.\u001b[39mlambd,\n\u001b[1;32m     17\u001b[0m             config\u001b[38;5;241m.\u001b[39mtask, a0, config\u001b[38;5;241m.\u001b[39mbg, bH, config\u001b[38;5;241m.\u001b[39mr, config\u001b[38;5;241m.\u001b[39mrho, \n\u001b[1;32m     18\u001b[0m             config\u001b[38;5;241m.\u001b[39mmax_iter, config\u001b[38;5;241m.\u001b[39mlog_freq, config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msketchysvrg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[43msketchysvrg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXtst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mopt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnystrom_pcg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     24\u001b[0m     nystrom_pcg(Xtr, ytr, Xtst, ytst, config\u001b[38;5;241m.\u001b[39mkernel_params, config\u001b[38;5;241m.\u001b[39mm, config\u001b[38;5;241m.\u001b[39mlambd,\n\u001b[1;32m     25\u001b[0m                 config\u001b[38;5;241m.\u001b[39mtask, a0, config\u001b[38;5;241m.\u001b[39mr, config\u001b[38;5;241m.\u001b[39mrho, config\u001b[38;5;241m.\u001b[39mmax_iter, \n\u001b[1;32m     26\u001b[0m                 config\u001b[38;5;241m.\u001b[39mlog_freq, config\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m, in \u001b[0;36msketchysvrg\u001b[0;34m(x, b, x_tst, b_tst, kernel_params, m, lambd, task, a0, bg, bH, r, rho, update_freq, max_iter, log_freq, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m K_nm_idx \u001b[38;5;241m=\u001b[39m get_kernel(x_idx_i, x_inducing_j, kernel_params)\n\u001b[1;32m     60\u001b[0m a_diff \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m-\u001b[39m a_tilde\n\u001b[0;32m---> 61\u001b[0m g_diff \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m/\u001b[39mbg \u001b[38;5;241m*\u001b[39m (K_nm_idx\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (\u001b[43mK_nm_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma_diff\u001b[49m)) \u001b[38;5;241m+\u001b[39m lambd \u001b[38;5;241m*\u001b[39m (K_mm \u001b[38;5;241m@\u001b[39m a_diff)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Apply the preconditioner\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m apply_nys_precond(U, S, rho, g_diff \u001b[38;5;241m+\u001b[39m g_bar)\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/lazy_tensor.py:2524\u001b[0m, in \u001b[0;36mGenericLazyTensor.__matmul__\u001b[0;34m(self, v, **kwargs)\u001b[0m\n\u001b[1;32m   2522\u001b[0m v_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlt_constructor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools\u001b[38;5;241m.\u001b[39mview(v, newdims))\n\u001b[1;32m   2523\u001b[0m Kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m*\u001b[39m v_  \u001b[38;5;66;03m# Supports broadcasting\u001b[39;00m\n\u001b[0;32m-> 2524\u001b[0m Kv \u001b[38;5;241m=\u001b[39m \u001b[43mKv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Matrix-vector or Matrix-matrix product\u001b[39;00m\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;66;03m# Expected behavior: if v is a vector, so should K @ v.\u001b[39;00m\n\u001b[1;32m   2527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools\u001b[38;5;241m.\u001b[39mview(Kv, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m Kv\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/lazy_tensor.py:2096\u001b[0m, in \u001b[0;36mGenericLazyTensor.sum\u001b[0;34m(self, axis, dim, **kwargs)\u001b[0m\n\u001b[1;32m   2094\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum\u001b[39m\u001b[38;5;124m\"\u001b[39m, dimres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2095\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/lazy_tensor.py:775\u001b[0m, in \u001b[0;36mGenericLazyTensor.reduction\u001b[0;34m(self, reduction_op, other, opt_arg, axis, dim, call, is_complex, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m     res\u001b[38;5;241m.\u001b[39mcallfun \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mGenred(\n\u001b[1;32m    765\u001b[0m         res\u001b[38;5;241m.\u001b[39mformula,\n\u001b[1;32m    766\u001b[0m         [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    772\u001b[0m         rec_multVar_highdim\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mrec_multVar_highdim,\n\u001b[1;32m    773\u001b[0m     )\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m call \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39msymbolic_variables) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m res\u001b[38;5;241m.\u001b[39m_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mres\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/lazy_tensor.py:957\u001b[0m, in \u001b[0;36mGenericLazyTensor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;66;03m# we replace by other\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mother\u001b[38;5;241m.\u001b[39mvariables[\u001b[38;5;241m0\u001b[39m],)\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/torch/generic/generic_red.py:687\u001b[0m, in \u001b[0;36mGenred.__call__\u001b[0;34m(self, backend, device_id, ranges, out, *args)\u001b[0m\n\u001b[1;32m    685\u001b[0m params\u001b[38;5;241m.\u001b[39mny \u001b[38;5;241m=\u001b[39m ny\n\u001b[1;32m    686\u001b[0m params\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m--> 687\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mGenredAutograd_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m postprocess(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction_op, nout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_arg, dtype)\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/torch/generic/generic_red.py:383\u001b[0m, in \u001b[0;36mGenredAutograd_fun\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGenredAutograd_fun\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGenredAutograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/torch/generic/generic_red.py:291\u001b[0m, in \u001b[0;36mGenredAutograd.forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39minputs):\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGenredAutograd_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/torch/generic/generic_red.py:121\u001b[0m, in \u001b[0;36mGenredAutograd_base._forward\u001b[0;34m(params, *args)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params\u001b[38;5;241m.\u001b[39mranges:\n\u001b[1;32m    116\u001b[0m     params\u001b[38;5;241m.\u001b[39mranges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    117\u001b[0m         r\u001b[38;5;241m.\u001b[39mto(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mranges\n\u001b[1;32m    119\u001b[0m     )\n\u001b[0;32m--> 121\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmyconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenred_pytorch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mny\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnbatchdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, torch\u001b[38;5;241m.\u001b[39mtensor([myconv\u001b[38;5;241m.\u001b[39mdimout, myconv\u001b[38;5;241m.\u001b[39mtagIJ])\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/keops_io/LoadKeOps.py:236\u001b[0m, in \u001b[0;36mLoadKeOps.genred\u001b[0;34m(self, device_args, ranges, nx, ny, nbatchdims, out, *args)\u001b[0m\n\u001b[1;32m    234\u001b[0m     out[:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_keops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mny\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpykeops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhalf2_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m postprocess_half2\n",
      "File \u001b[0;32m~/fast_krr/fast_krr_env/lib/python3.10/site-packages/pykeops/common/keops_io/LoadKeOps_nvrtc.py:48\u001b[0m, in \u001b[0;36mLoadKeOps_nvrtc_class.call_keops\u001b[0;34m(self, nx, ny)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_keops\u001b[39m(\u001b[38;5;28mself\u001b[39m, nx, ny):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_keops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtagHostDevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mny\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtagI\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtagZero\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_half\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag1D2D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_block_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_chunk_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindsi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindsj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindsp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimsx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimsy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdimsp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mranges_ptr_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_ptr_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margshapes_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with wandb.init(project=wandb_project, config=experiment_args):\n",
    "    # Access the experiment configuration\n",
    "    config = wandb.config\n",
    "\n",
    "    # Load the dataset\n",
    "    Xtr, Xtst, ytr, ytst = load_data(config.dataset, config.seed, config.device)\n",
    "\n",
    "    bH = int(Xtr.shape[0] ** 0.5)\n",
    "\n",
    "    # Initialize at 0\n",
    "    a0 = torch.zeros(config.m, device=config.device)\n",
    "\n",
    "    # Run the optimizer\n",
    "    with torch.no_grad():\n",
    "        if config.opt == 'sketchysgd':\n",
    "            sketchysgd(Xtr, ytr, Xtst, ytst, config.kernel_params, config.m, config.lambd,\n",
    "                    config.task, a0, config.bg, bH, config.r, config.rho, \n",
    "                    config.max_iter, config.log_freq, config.device)\n",
    "        elif config.opt == 'sketchysvrg':\n",
    "            sketchysvrg(Xtr, ytr, Xtst, ytst, config.kernel_params, config.m, config.lambd,\n",
    "                        config.task, a0, config.bg, bH, config.r, config.rho, config.update_freq,\n",
    "                        config.max_iter, config.log_freq, config.device)\n",
    "        elif config.opt == 'nystrom_pcg':\n",
    "            nystrom_pcg(Xtr, ytr, Xtst, ytst, config.kernel_params, config.m, config.lambd,\n",
    "                        config.task, a0, config.r, config.rho, config.max_iter, \n",
    "                        config.log_freq, config.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_krr_env",
   "language": "python",
   "name": "fast_krr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
